# 梯度下降总结
@(pytorch与深度学习)[机器学习|梯度下降]

---------------------------
[TOC]

## 数据噪声
**问题1：**有一些数据点，假设这些数据点的集合通过横纵坐标表示为${\lbrace(x_i,y_i),i\in(0...n)\rbrace}$,我们想通过这些点得到他的数据表达式模型，简单一假设模型为线性模型$y=wx+b$为例子,那么超参数**w**和**b**是未知量，很自然我们会想到解决方法---两个方程两个未知数，我们只需要两个点就可以得到两个方程从而求出**w**和**b**，那么为什么一般讲到回归问题我们需要很多点？即便用两个点得到的方程来计算得到的直线方程但是却存在不能很好的表示其他所有的点的问题，从数学角度，怎么去看待求解方程得到的模型不能适配所有模型？怎么去衡量我们的估计模型尽可能表示所有点这种特性的好坏？


**回答1：**在一些我们得到的实际点中，真实模型生成这些点的过程中会存在一定的噪声$\varepsilon$,站在上帝视角来看，假设生成点的真实的数据模型是$y=wx+b$,但是在生成点的过程中有噪声的原因，导致我们的点其实符合模型为（噪声符合某分布，这里可假设噪声符合标准正态分布）：      
$$y=wx+b+\varepsilon,\varepsilon\sim N(0,1)\tag{1-1}$$

这样我们看到的数据其实在原数据模型基础上多了噪声，也就是说我们看到的数据点和上帝通过上帝视角看到的数据模型生成的点其实还隔着噪声，噪声属于随机变量，不能精确求解，所以使用解方程的思想来对待利用数据点求解真实模型的问题就不那么行的通了，这时考虑用概率统计的思想来逼近真实的数据生成模型。我们知道，**我们观测的点**符合式子（1-1）表示的模型，即对任意的$(x_i,y_i)$,总是有
$$y_i=wx_i+b+\epsilon_i\tag{1-2}$$

假设我们通过这些数据点得到的估计数据模型为
$$y=w_0x+b_0\tag{1-3}$$
那么我们的目标就是对于任意的$(x_i,y_i)$,使得$w_0x_i+b_0\longrightarrow wx_i+b+\epsilon_i$,即$w_0x_i+b_0\longrightarrow y_i$,就是说对于任意的$x_i$，我们总是希望通过我们模型给出的$y_{i} '\longrightarrow y_{i}$。($\longrightarrow $在这里表示趋近于)
将问题用等价的数学表示即$\sum_{i=0}^{n} (w_0x_i+b_0-y_i)^2\longrightarrow 0$，为了方便求导运算，所以我们将问题等价于，假设存在函数$loss=(\frac{1}{n}\sum_{i=0}^{n} (wx_i+b-y_i)^2)$，求$loss$取得最小值时的$w$和$b$。
>**一般化：**针对观察到的数据假设我们给出的数据模型为$f(x)$,则$loss=\frac{1}{n}\sum_{i=0}^{n} (f(x_i)-y_i)^2$,大家把这时的$loss$称为经验风险(empirical risk),把目标求得$loss$的最小值称为经验风险最小化。结构经验风险化的概念待续...先给出笔者概括的说法，我们的方法，具有他的局限性，该方法可以使得$f(x)$逼近上帝视角的$y=wx+b$模型，但是无发达到精确描述，原因就是噪声是随机变量，随机事件的发生结果不可以精确描述，就像你不可以预测彩票的中奖号码，当你使用最小化$loss=(\frac{1}{n}\sum_{i=0}^{n} (wx_i+b-y_i)^2)$时，其实总共大致可以分成两个过程，过程一首先逼近上帝视角的模型，过程二贪心的想把随机变量$\epsilon$也囊括在我们的模型里面，当你的模型把带有噪声的数据点都囊括得很好，那你离真实的模型渐行渐远，换句话来说，$\epsilon$针对上帝的模型进行了修饰，原来的模型可能是蛇，经过$\epsilon$外衣修饰后表现出来的东西是带有四条腿的龙，当你通过你的模型想要精确勾勒了一条龙，实际上却对原来勾勒蛇的目标恰恰起到了画蛇添足的效果。所以我们使用的这个方法存在一个合理的度，因为数据存在噪声，这些数据只能作为原始模型的轮廓，他大致反映原始模型，原始模型生成的数据点和我们观测的数据误差$\pm\epsilon$。当你使用这个方法利用已有的数据点想要超出$\epsilon$精度的去描述真实模型，无异于有一个GPS定位系统精度是米级别，每秒传来的数据在米级别范围内震荡你却用它来做到毫米厘米的运动轨迹预测，你得到的结论说目标物一直在动啊，其实他一直静止在睡觉，真正在动的是**噪声**，合理的度就是当物体长时间只在米级别内震荡我们就认为他是静止的。

------

## 梯度...