# 梯度下降总结
@(pytorch与深度学习)[机器学习, 梯度下降，TODO]

---------------------------
[TOC]

## 数据噪声
**问题1：**有一些数据点，假设这些数据点的集合通过横纵坐标表示为${\lbrace(x_i,y_i),i\in(0...n)\rbrace}$,我们想通过这些点得到他的数据表达式模型，简单一假设模型为线性模型$y=wx+b$为例子,那么超参数**w**和**b**是未知量，很自然我们会想到解决方法---两个方程两个未知数，我们只需要两个点就可以得到两个方程从而求出**w**和**b**，那么为什么一般讲到回归问题我们需要很多点？即便用两个点得到的方程来计算得到的直线方程但是却存在不能很好的表示其他所有的点的问题，从数学角度，怎么去看待求解方程得到的模型不能适配所有模型？怎么去衡量我们的估计模型尽可能表示所有点这种特性的好坏？


**回答1：**在一些我们得到的实际点中，真实模型生成这些点的过程中会存在一定的噪声$\varepsilon$,站在上帝视角来看，假设生成点的真实的数据模型是$y=wx+b$,但是在生成点的过程中有噪声的原因，导致我们的点其实符合模型为（噪声符合某分布，这里可假设噪声符合标准正态分布）：      
$$y=wx+b+\varepsilon,\varepsilon\sim N(0,1)\tag{1-1}$$

这样我们看到的数据其实在原数据模型基础上多了噪声，也就是说我们看到的数据点和上帝通过上帝视角看到的数据模型生成的点其实还隔着噪声，噪声属于随机变量，不能精确求解，所以使用解方程的思想来对待利用数据点求解真实模型的问题就不那么行的通了，这时考虑用概率统计的思想来逼近真实的数据生成模型。我们知道，**我们观测的点**符合式子（1-1）表示的模型，即对任意的$(x_i,y_i)$,总是有
$$y_i=wx_i+b+\epsilon_i\tag{1-2}$$

假设我们通过这些数据点得到的估计数据模型为
$$y=w_0x+b_0\tag{1-3}$$
那么我们的目标就是对于任意的$(x_i,y_i)$,使得$w_0x_i+b_0\longrightarrow wx_i+b+\epsilon_i$,即$w_0x_i+b_0\longrightarrow y_i$,就是说对于任意的$x_i$，我们总是希望通过我们模型给出的$y_{i} '\longrightarrow y_{i}$。($\longrightarrow $在这里表示趋近于)
将问题用等价的数学表示即$\sum_{i=0}^{n} (w_0x_i+b_0-y_i)^2\longrightarrow 0$，为了方便求导运算，所以我们将问题等价于，假设存在函数$loss=(\frac{1}{n}\sum_{i=0}^{n} (wx_i+b-y_i)^2)$，求$loss$取得最小值时的$w$和$b$。
>**一般化：**针对观察到的数据假设我们给出的数据模型为$f(x)$,则$loss=\frac{1}{n}\sum_{i=0}^{n} (f(x_i)-y_i)^2$,大家把这时的$loss$称为经验风险(empirical risk),把目标求得$loss$的最小值称为经验风险最小化。结构经验风险化的概念待续...先给出笔者概括的说法，我们的方法，具有他的局限性，该方法可以使得$f(x)$逼近上帝视角的$y=wx+b$模型，但是无发达到精确描述，原因就是噪声是随机变量，随机事件的发生结果不可以精确描述，就像你不可以预测彩票的中奖号码，当你使用最小化$loss=(\frac{1}{n}\sum_{i=0}^{n} (wx_i+b-y_i)^2)$时，其实总共大致可以分成两个过程，过程一首先逼近上帝视角的模型，过程二贪心的想把随机变量$\epsilon$也囊括在我们的模型里面，当你的模型把带有噪声的数据点都囊括得很好，那你离真实的模型渐行渐远，换句话来说，$\epsilon$针对上帝的模型进行了修饰，原来的模型可能是蛇，经过$\epsilon$外衣修饰后表现出来的东西是带有四条腿的龙，当你通过你的模型想要精确勾勒了一条龙，实际上却对原来勾勒蛇的目标恰恰起到了画蛇添足的效果。所以我们使用的这个方法存在一个合理的度，因为数据存在噪声，这些数据只能作为原始模型的轮廓，他大致反映原始模型，原始模型生成的数据点和我们观测的数据误差$\pm\epsilon$。当你使用这个方法利用已有的数据点想要超出$\epsilon$精度的去描述真实模型，无异于有一个GPS定位系统精度是米级别，每秒传来的数据在米级别范围内震荡你却用它来做到毫米厘米的运动轨迹探测，你得到的结论说目标物一直在动啊，其实他一直静止在睡觉，真正在动的是**噪声**。这种情况下，合理的度比方说当物体长时间只在米级别内震荡我们就认为他是静止的。

------

## 梯度方向与标准方程
同样以$loss=(\frac{1}{n}\sum_{i=0}^{n} (wx_i+b-y_i)^2)$为例，为了最小化该函数，我们需要针对该函数分别对$w$和$b$求偏导,得到
$$\frac{\partial loss}{\partial w}=(\frac{2w}{n}*\sum_{i=0}^n(wx_i+b-y_i))\tag{2-1}$$
$$\frac{\partial loss}{\partial b}=(\frac{2}{n}*\sum_{i=0}^n(wx_i+b-y_i))\tag{2-2}$$
### 标准方程法
这时候，大家肯定会想直接通过$\frac{\partial loss}{\partial w}=0$和$\frac{\partial loss}{\partial b}=0$直接求出最小的$loss$时$w$和$b$的值：
即求解$$(\frac{2w}{n}*(w*\sum_{i=0}^nx_i+nb-\sum_{i=0}^ny_i))=0\tag{2-3}$$
$$(\frac{2}{n}*(w*\sum_{i=0}^nx_i+nb-\sum_{i=0}^ny_i))=0\tag{2-4}$$
得到解$$w=0,w=\frac{\sum_{i=0}^ny_i-nb}{\sum_{i=0}^nx_i}$$
$$b=\frac{\sum_{i=0}^ny_i-w*\sum_{i=0}^nx_i}{n}$$
>这种方法我们把它称为`标准方程法`，我们看到最后由于$w$和$b$无法直接求解。事实上，有些情况能够用这种方法，有一些情况不能。这一部分会需要用到`正规方程`的知识，后期对这一块知识，我会在本篇的最后有所补充，讨论看看**什么时候用上面的方法能得到直接求出$w_t$和$b_t$的全局最优**，并将结论一般化(Normal equation)。因为需要用到矩阵，会需要一定篇幅展开，不利于机器学习求解这类问题的主流方法梯度下降的讨论，所以我在这里先留白，我们只需要知道这种方法在我们文章的例子中不能解决问题，所以我们来看看另外一种方法——**梯度下降**。
### 梯度方向下降
针对任意$w_t$和$b_t$,我们总能通过式(2-1)和式(2-2)知道$w$和$b$为了使得$loss$更小所移动的方向，这样，我们只需要每次都使得新的$w_{t+1}$和$b_{t+1}$在原来$w_t$和$b_t$的基础上朝着$loss$更小所移动的方向移动一小步，这样一直进行下去，只要这个函数是**凸函数**，那么最后$w_{t+1}$和$b_{t+1}$就会收敛到较优解。
即

$$w_{t+1}=w_t-\alpha*\frac{\partial loss}{\partial w}\tag{2-5}$$

$$b_{t+1}=b_t-\alpha*\frac{\partial loss}{\partial b}\tag{2-6}$$
经过长时间的迭代后，试想如果每次都向全局最优前进，那么最终我们得到的$w_t$和$b_t$就会收敛在全局最优。
即
![@梯度下降示例 | center | 300x200](./1550764644926.png)

------

## 梯度下降的分类
之前我们看到的式(2-1)和式(2-2)是梯度下降最原始的形式，即每一次更新$w_{t+1}$和$b_{t+1}$取值时都使用所有的样本进行更新，即对式(2-5)和式(2-6)更新$\frac{\partial loss}{\partial w}$，原式(2-5)和式(2-6)变为
$$w_{t+1}=w_t-\alpha*(\frac{2w_t}{n}*\sum_{i=0}^n(w_tx_i+b_t-y_i))\tag{3-1}$$
$$b_{t+1}=b_t-\alpha*(\frac{2}{n}*\sum_{i=0}^n(w_tx_i+b_t-y_i))\tag{3-2}$$
每次更新$w_{t+1}$和$b_{t+1}$，都会分别重新更新计算$\frac{2w_t}{n}*\sum_{i=0}^n(w_tx_i+b-y_i)$和$\frac{2}{n}*\sum_{i=0}^n(w_tx_i+b-y_i)$,n是训练数据集的总体数目，这种更新$w_{t+1}$和$b_{t+1}$的方法我们也把他称为**批量梯度下降**。
### 批量梯度下降法(BGD)
随机梯度下降法,英文名Batch Gradient Descent,从上面的公式，可以注意到，这种每次更新变量值的方法，都会计算训练集并用到所有的数据,那么在普通算法实现这种方法时间复杂度为$O(m*n)$,n为数据集规模,m为迭代次数。如果样本数目很大，比如亿级，那么这种方法的迭代数据肯定效果上是有所欠缺的。所以，为了迭代的速度，这就引入了另外一种方法，**随机梯度下降**。
![@BGD迭代收敛曲线 | center | 280x255](./1550893919964.png)

### 随机梯度下降法(SGD)
说是一种新的方法,其实就是在更新式子(3-1)和(3-2)时，为了提升速度，做一些偷懒的改动，我们看看做了哪些改动。首先,为了便于比较我们将式子(3-1)和(3-2)修改成

$$w_{t+1}=w_t-\alpha*(2w_t(w_t\overline x+b_t-\overline y))\tag{3-3}$$
$$b_{t+1}=b_t-\alpha*(2(w_t\overline x+b_t-\overline y))\tag{3-4}$$

因为所有的点都满足(1-1)提到的模型，那么我们将之前式子(1-1)带入，得到
$$w_{t+1}=w_t-\alpha*(2w_t((w_t-w)\overline x+(b_t-b)+\mu))\tag{3-5}$$
$$b_{t+1}=b_t-\alpha*(2((w_t-w)\overline x+(b_t-b)+\mu))\tag{3-6}$$
我们知道因为$\varepsilon\sim N(0,1)$,所以$\mu=0$。
这时,在$\alpha$,$w_t$,$w$,$b_t$,$b$都固定的情况下,$w_{t+1}$在$w_t$的基础上使得$loss$降低的方向移动多少，就和$\overline x$有关，有的$x_i$使得$w_t$移动的多，有的$x_i$使得$w_t$移动的少。不考虑噪声，从数据集随机选取$x_i$来代表$\overline x$,这样子，使得每次并不一定是朝着全局最优的方式，但是由于每一次只需要一个数据点，能够大大节省资源和时间。因为使得$x_i\approx\overline x$，这样采样计算方法受到噪声的影响比较大，和最后的全局最优可能会有一些距离，于是为了折中速度和采样计算方法的噪声影响，便有了**小批量梯度下降法**。
![@SGD迭代收敛曲线 | center | 280x255](./1550894064976.png)

### 小批量梯度下降法(MBGD)
和上面SGD的方法一样，不同的是，为了减少噪声的影响，小批量梯度下降法每次对式子(3-3)更新迭代时，会多采样一些点。比如每次迭代更新采样10个点，假设采样数据集为$\{x_i',i\in(1,2,...10)\}$,我们则是用$\overline {x_i'}$来代替$\overline x$,原来的式(3-3)和式(3-4)变为
$$w_{t+1}=w_t-\alpha*(2w_t(w_t\overline {x_i'}+b_t-\overline y))\tag{3-7}$$
$$b_{t+1}=b_t-\alpha*(2(w_t\overline {x_i'}+b_t-\overline y))\tag{3-8}$$
通过这种批采样方法来更新$w_{t+1}$可以达到在迭代速度和采样噪声影响上取得折衷的效果。
### 梯度下降分类总结

| - | BGD | SGD | MBGD |
| :------: | :-------- | :--------| :------ |
|优点| 全局最优解；易于并行实现|训练速度快 |折衷 |
|缺点| 当样本数目很多时，训练过程会很慢|准确度下降，并不是全局最优；不易于并行实现|  折衷|


------

## 梯度下降的均一化

仔细思考，在梯度下降的过程中，不同的变量共用了学习率$\alpha$，直观上来看，学习率可以类比下降的步长，所有的变量共用了步长，当然规模量级小的会更快收敛到较优解。好比同样一个人相同的速度跑100米和跑1000米，在前者花的时间肯定更少。而且针对未规模化的数据，选择的学习率过小，规模量级大的1000米会导致整体收敛时间过大；选择的学习率过大会导致规模量级小的在较优解附近震荡。所以，将所有维度规模化到同一规模，有利于进行学习率步长更好的选择。

##标准方程法补充
### 正规方程
第一次看到这个概念是在Andrew的视频学习梯度下降中，Andrew给出了我们本篇文章中所提到的$w$和$b$的直接求解的方法---正规方程。为了方便，我在这里对原来我们的$y=wx+b$的表达做一些变换。

###梯度下降与正规方程总结