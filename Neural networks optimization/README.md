# 神经网络优化算法总结
@(pytorch与深度学习)[机器学习, Pytorch, TODO]

---------------------------
[TOC]

写一些梯度下降的优化算法Adam  之类的
##lr与动量
###动量

原来的梯度更新公式为$$w^{k+1}=w^k-\alpha\nabla f(w^k)\tag{1-1}$$
我们观察这个式子，发现$-\alpha \nabla f(w^k)$其实就表示朝着梯度的方向前进。
引入动量的思想，将参数迭代更新的式子变成:
 $$z^{k+1}=\beta z^k+\nabla f(w^k)\tag{1-2}$$

$$w^{k+1}=w^k-\alpha z^{k+1}\tag{1-3}$$

仔细观察,我们将式子(1-2)代入式子(1-3)我们得到:
$$w^{k+1}=w^k-\alpha\nabla f(w^k)-\alpha\beta z^k\tag[1-4]$$
里面的$-\alpha\beta z^k$就是使得当前的参数更新，不仅考虑当前梯度，同时考虑之前的梯度方向，越远之前迭代的梯度方向，对当前的影响越小, 反之影响越大。
![@未引入动量原始更新式子|center](./1552040977724.png)
在非BGD梯度下降的更新方法中,我们看到该网络没有找到最优解，更新方向非常随机，仅仅考虑当下的输入，没有考虑历史的效应，使得梯度更新的方向非常尖锐，随机。其实思想上，有些像`指数平滑`的思想，我们知道因为在非BGD的更新方式中，梯度方向和全局梯度方向其实还隔着一个采样噪声，所以引入指数平滑历史梯度方向的方法来减少这种噪声对梯度更新的影响。
####pytorch使用动量
![momentum demo|center](./1552041521073.png)

###lr
监听耐心值
![耐心监听方法](./1552041850571.png)
直接设定每训练多少epoch就降低
![一如既往方法](./1552042068832.png)
